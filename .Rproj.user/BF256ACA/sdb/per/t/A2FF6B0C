{
    "contents" : "# code to run full example of rare events logit form event histroy models for cross validation\nrm(list = ls())\n# make sure you have downloaded the biglm and snowfall packages\n\n# EHA Regression Function that will run an individual regression\n\nEHA_Reduced_Reg <- function(index){\n    \n    #need to change this to the folder where you are storing workshop materials\n    setwd(\"~/Dropbox/RA_and_Consulting_Work/ICPSR_Summer_14/HPC_Workshop_Materials\")\n    \n    #get the lookup from the EHA_congress_number_split_indexes.csv file\n    Congress_Split_Lookup <- read.csv(\"EHA_lookup.csv\", header = F, stringsAsFactors = FALSE)\n    \n    #which congress are we looking at\n    congress_number <- Congress_Split_Lookup[index,1]\n    #which cross validation split are we working with\n    split <- Congress_Split_Lookup[index,2]\n    #which independent varaible are we using (add five bcause first five are the same for all models)\n    var <- Congress_Split_Lookup[index,3] + 5\n    # add 92 to get the actual congress number\n    congress_number <- 92+congress_number\n    \n    #load the data\n    load(file = paste(\"EHA_Reduced_Data_\",split,\"_\",congress_number,\".Rdata\", sep = \"\"))\n    \n    #load the weights for rare events logit weights\n    load(file = paste(\"Weights_\",split,\"_\",congress_number,\".Rdata\", sep = \"\"))\n    \n    #make sure that data is in a data.frame\n    reduced_data <- as.data.frame(reduced_data)\n    #Turn the bill number into a factor variable\n    reduced_data$V4 <- as.factor(reduced_data$V4)\n    #senator Specific Factors     \n    reduced_data$V2 <- as.factor(reduced_data$V2)\n    \n    #add in weights to dataset \n    reduced_data$V3 <- weights\n    #create the logit formula where: whether a Senator cosponsors in this time period as function of a senator dummy, a bill dummy, time and time squared effects and the number of influencers in the current network that had already cosponsored the bill.\n    \n    #full specification (will require lots of RAM and run much slower)\n    #str <- paste(\"V1 ~ V2 + V4 + V5 + V5^2 + V\",var,sep = \"\")\n    \n    #reduced specification for example\n    str <- paste(\"V1 ~  V5 + V5^2 + V\",var,sep = \"\")\n    #run the model\n    model<- bigglm(as.formula(str), data = reduced_data, family=binomial(),weights = ~V3, maxit = 30) \n    #get the log likelihood\n    loglik<- (-.5*model$deviance) \n    return(loglik)\n}\n\n\n\n# Function to run batch EHA Regressions\n\nRun_Batch_EHA_Regressions <- function(start = 5, end = 5,numedges = 10, splits = 10, numcpus = 6){\n    \n    #need to change this to the folder where you are storing workshop materials\n    setwd(\"~/Dropbox/RA_and_Consulting_Work/ICPSR_Summer_14/HPC_Workshop_Materials\")\n    \n    #have we already saveed out progress\n    already <- FALSE\n    \n    #the indexes of session of the senate to run over -- we only have the 5 session in my dataset for this exercise but in the paper I go over 12 session\n    vector <- c(start:end)\n    #loop over each Congress\n    for(i in vector){\n        print(paste(\"Currently on Senate Number: \",i+92,sep = \"\"))\n        \n        #first index in the current senate \n        st <- numedges*(i-5)+1\n        #last index in the current senate\n        nd <- numedges*splits*(i-4)\n        #these are all of the rows in the EHA_lookup.csv file that we will be looking at\n        variable_indexes <- st:nd\n        \n        ############ now get ready for paralell computing\n        #load our packages\n        library(snowfall)\n        library(biglm)\n        #intitalizes snowfall session \n        sfInit(parallel=TRUE, cpus=numcpus )\n        \n        #check to see if we are running in parallel\n        if(sfParallel())\n            cat( \"Running in parallel mode on\", sfCpus(), \"nodes.\\n\" )\n        else\n            cat( \"Running in sequential mode.\\n\" )\n        \n        #export all packages and libraries currently loaded in workspace\n        for (i in 1:length(.packages())){\n            eval(call(\"sfLibrary\", (.packages()[i]), character.only=TRUE))\n        }\n        \n        # apply our problem across the cluster using hte indexes we have determined and load balancing\n        cur_result <- sfClusterApplyLB(variable_indexes,EHA_Reduced_Reg)\n        \n        #stop the cluster when we are done -- this is very important and must be done manually every time\n        sfStop()\n        \n        \n        #if we have not already saved our data this time around, save it!\n        if(!already){\n            result <- cur_result\n            save(result, file = \"Results.Rdata\")\n            already <- TRUE\n        }else{\n            result <- rbind(result,cur_result)\n            save(result, file = \"Results.Rdata\")\n        }\n        \n    }\n    #spit back our results\n    return(result) \n}\n\n\n# run cross validation using a cluster\nsystem.time({\n    results <- Run_Batch_EHA_Regressions(numcpus = 6)\n})\n\n\n\n#take a look!\ncolors <- rep(1,10)\nfor(i in 2:10){\n    colors <- c(colors,rep(i,10))  \n}\nplot(unlist(results),col = colors, pch = 19,ylab = \"likelihood\", main = \"Likelihood surfaces for 10 splits of data from 97th Senate\")\n\n\n",
    "created" : 1433270559651.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "4123759271",
    "id" : "A2FF6B0C",
    "lastKnownWriteTime" : 1433266339,
    "path" : "~/GitHub/ISSR_Data_Science_Summer_Summit_15/Scripts/HPC_Example.R",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 6,
    "source_on_save" : false,
    "type" : "r_source"
}