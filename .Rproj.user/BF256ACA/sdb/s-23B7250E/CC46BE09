{
    "contents" : "# This assignment is pretty simple -- scrape the full text of 100 bills \n# inrtroduced in the Senate in the 112th congress and count the number of unique\n# words.  Once you have done this, I encourage you to go further, look for key\n# words and try to make some cool looking output. Style points are where it is \n# at!\n\n\n# Follow the template below but copy and paste all of the code into a script \n# file that you push to your Github repo after completing each step.\n\nrm(list = ls())\n# load the necessary libararies\nlibrary(stringr)\nlibrary(scrapeR)\n\n# Load in the bill urls -- you may need to set your working directory or alter \n# the path below\nload(\"Bill_URLs.Rdata\")\n\n# Try visiting the webiste, you will see that these URL's are from a beta \n# version. The URLs will look like:\n# http://beta.congress.gov/bill/112th-congress/senate-bill/886\n# What we actually want is something of the form:\n# https://www.congress.gov/bill/112th-congress/senate-bill/886/text?format=txt\n# we will need to loop through the text and replace the beginning \"http://beta.\"\n# with \"https://www.\" and then we will need to paste on \"/text?format=txt\" at \n# the end of each string. \n\n\n\n\n# You will want to use str_replace() and paste() functions to do this in a for()\n# loop.\n\nbill_urls_fixed <- rep(\"\",100)\nfor  (i in 1:100) {\n  bill_urls_fixed[i]<- str_replace(Bill_URLs[i], \"http://beta\",\"https://www\")\n  bill_urls_fixed[i]<- paste(bill_urls_fixed[i], \"/text?format=txt\", sep=\"\")\n }\n\n\n\n# Once you have the right URLs, you will want to scrape the web pages. Lets \n# start with a function adapted from the intermediate workshop:\nscrape_page <- function(url){\n  \n  # Print out the input name\n  cat(url, \"\\n\")\n  \n  # Make the input name all lowercase\n  url <- tolower(url)\n  \n  # Downloads the web page source code\n  page <- getURL(url, .opts = list(ssl.verifypeer = FALSE))\n  \n  # Split on newlines\n  page <- str_split(page,'\\n')[[1]]\n  \n  # Start of bill text \n  start <- grep(\"112th CONGRESS\",page)\n  \n  # End of bill text\n  end <- grep(\"&lt;all&gt;\",page)\n  \n  # Get just the text\n  if(length(end) > 0 & length(start) > 0){\n    # Get just the text\n    print(start)\n    print(end)\n    if(!is.na(start) & !is.na(end)){\n      if(start < end & start > 0 & end > 0){\n        bill_text <- page[start:end]\n      }else{\n        bill_text <- \"\"\n      }\n    }else{\n      bill_text <- \"\"\n    }\n  }else{\n    bill_text <- \"\"\n  }\n  \n  # Save to a named list object\n  to_return <- list(page = page, text = bill_text)\n  \n  # return the list\n  return(to_return)\n}\n\n# test it out, take a look at the \ntest <- scrape_page( url = \"https://www.congress.gov/bill/112th-congress/senate-bill/886/text?format=txt\")\n\n# Now you will need to create a list object to store the data in, and loop over \n# URLS to store the data in the list. You will probably want to save your data\n# as an .Rdata object using save() at this point. One important point is that \n# you NEED TO INCLUDE a Sys.sleep(5) in your scraping loop so you do not go too\n# fast and overwhelm the congress.gov servers. Going too fast can land you in \n# BIG legal trouble (that is called a \"denial of service attack\") so jsut keep \n# things at a reasonable pace. \n\n\n\nbill.text.list <- vector(\"list\",length = 100)\nfor (i in 1:100){\n  temp<-scrape_page(url = bill_urls_fixed[i])\n  bill.text.list[[i]] <-temp\n  Sys.sleep(4)\n}\n \n\nsave(bill.text.list, file =\"Scraped Bill Text.Rdata\")\n\n# when needed, re load it in.\nload (\"Scraped Bill Text.Rdata\")\n\n\n\n\n# Now you will need to deal with the text... This function is being given to you\n# as a way to clean up a single string.\n\n# test string <- \"inspections..#$^relocation..???!!!}{[]()\"\n\n\nClean_String <- function(string){\n  # Lowercase\n  temp <- tolower(string)\n  # Remove everything that is not a number letter ? or !\n  temp <- stringr::str_replace_all(temp,\"[^a-zA-Z\\\\s:\\\\?\\\\!]\", \" \")\n  # Shrink down to just one white space\n  temp <- stringr::str_replace_all(temp,\"[\\\\s]+\", \" \")\n  # Split it\n  temp <- stringr::str_split(temp, \" \")[[1]]\n  # Get rid of trailing \"\" if necessary\n  indexes <- which(temp == \"\")\n  if(length(indexes) > 0){\n    temp <- temp[-indexes]\n  }\n  return(temp)\n}\n\n\n#' function to clean text\nClean_Text_Block <- function(text){\n  if(length(text) <= 1){\n    # Check to see if there is any text at all with another conditional\n    if(length(text) == 0){\n      cat(\"There was no text in this bill! \\n\")\n      to_return <- list(num_tokens = 0, unique_tokens = 0, text = \"\")\n    }else{\n      # If there is , and only only one line of text then tokenize it\n      clean_text <- Clean_String(text)\n      num_tok <- length(clean_text)\n      num_uniq <- length(unique(clean_text))\n      to_return <- list(num_tokens = num_tok, unique_tokens = num_uniq, text = clean_text)\n    }\n  }else{\n    # Get rid of blank lines\n    indexes <- which(text == \"\")\n    if(length(indexes) > 0){\n      text <- text[-indexes]\n    }\n    # Loop through the lines in the text and use the append() function to \n    clean_text <- Clean_String(text[1])\n    for(i in 2:length(text)){\n      # add them to a vector \n      clean_text <- append(clean_text,Clean_String(text[i]))\n    }\n    num_tok <- length(clean_text)\n    num_uniq <- length(unique(clean_text))\n    to_return <- list(num_tokens = num_tok, unique_tokens = num_uniq, text = clean_text)\n  }\n  # Calculate the number of tokens and unique tokens and return them in a \n  # named list object.\n  return(to_return)\n}\n\n#' run on all bills\nclean_bill_text <- vector(mode = \"list\",length = 100)\nfor(i in 1:100){\n  print(i)\n  clean_bill_text[[i]] <- Clean_Text_Block(bill.text.list[[i]]$text)\n}\n\n#' calculate: total_token_count, total_unique_words\n\ntotal_token_count <- 0\nall_tokens <- NULL\nfor(i in 1:100){\n  print(i)\n  #keep appending the tokens to a giant vector\n  all_tokens <- append(all_tokens,clean_bill_text[[i]]$text)\n  total_token_count <- total_token_count + clean_bill_text[[i]]$num_tokens\n}\n# get unique words\nunique_words <- unique(all_tokens)\ntotal_unique_words <- length(unique_words)\ncat(\"There were a total of\",total_token_count,\"tokens used in all documents and the number of unique words is:\",total_unique_words,\" \\n\" )\n\n\n####### Stretch goals #######\n# So, you finished all of that, what next? Here are some other useful things you\n# should try to do.\n\n# 1. Write a function that counts the number of times a given word appears in \n# all 100 bills.\n\n#######neither of the two attpemst below worked.  \n# leaving here as evidence of what I tried\n\n\nwordcount<- function(word){\n  tally <-0\n  for(i in 1:100) {\n       if (grepl(word, bill.text.list[[i]]$text)) {\n         tally<- (tally+1)\n       }\n  }\nreturn(tally)\n}\nwordcount<- function(word){\n  for(i in 1:100) {\n    tally <- str_count(bill.text.list[[i]]$text, word)\n  }\n  return(tally)\n  }\n\nwordcount(word = \"policy\")\n\n  \n\n\n# 2. Take a look at the raw html for a bill and try to write a function that \n# that will extract some other pieces of metadata from it (such as the date\n# it was introduced, the author, and whether it made it to the floor) and then\n# save all of that data into another dataframe.\n# 3. Generate a dataframe with two columns, the first has a word and the second\n# has the number of times it appears.\n# 4. Create an dotplot using ggplot2 showing differences in the use of some \n# word(s) across different bills by some descriptive feature (perhaps author \n# party?)\n\n\n\n\n",
    "created" : 1433185734207.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "308286275",
    "id" : "CC46BE09",
    "lastKnownWriteTime" : 1433206785,
    "path" : "~/GitHub/ISSR_Data_Science/scripts/Day_One_Exercise.R",
    "project_path" : "scripts/Day_One_Exercise.R",
    "properties" : {
    },
    "relative_order" : 5,
    "source_on_save" : false,
    "type" : "r_source"
}