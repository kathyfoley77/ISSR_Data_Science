my_string_vector <- str_split(my_string, "!")[[1]]
print(my_string_vector)
grep("\\?",my_string_vector)
grepl("\\?",my_string_vector[1])
str_replace_all(my_string, "e","___")
str_extract_all(my_string,"[0-9]+")
rm(list = ls())
# load the necessary libararies
library(stringr)
library(scrapeR)
load("./Data/Bill_URLs.Rdata")
getwd()
load("Bill_URLs.Rdata")
Bill_URLs
bill_urls_fixed <- rep("",100)
for  (i in 1:100) {
bill_urls_fixed[i]<- str_replace(Bill_URLs[i], "http://beta","https://www")
}
bill_urls_fixed
bill_urls_fixed <- rep("",100)
for  (i in 1:100) {
bill_urls_fixed[i]<- str_replace(Bill_URLs[i], "http://beta","https://www")
bill_urls_fixed[i]<- paste(ill_urls_fixed[i], "/text?format=txt", sep="")
}
bill_urls_fixed <- rep("",100)
for  (i in 1:100) {
bill_urls_fixed[i]<- str_replace(Bill_URLs[i], "http://beta","https://www")
bill_urls_fixed[i]<- paste(bill_urls_fixed[i], "/text?format=txt", sep="")
}
bill_urls_fixed
test <- scrape_page( url = "https://www.congress.gov/bill/112th-congress/senate-bill/886/text?format=txt")
# Once you have the right URLs, you will want to scrape the web pages. Lets
# start with a function adapted from the intermediate workshop:
scrape_page <- function(url){
# Print out the input name
cat(url, "\n")
# Make the input name all lowercase
url <- tolower(url)
# Downloads the web page source code
page <- getURL(url, .opts = list(ssl.verifypeer = FALSE))
# Split on newlines
page <- str_split(page,'\n')[[1]]
# Start of bill text
start <- grep("112th CONGRESS",page)
# End of bill text
end <- grep("&lt;all&gt;",page)
# Get just the text
bill_text <- page[start:end]
# Save to a named list object
to_return <- list(page = page, text = bill_text)
# return the list
return(to_return)
}
test <- scrape_page( url = "https://www.congress.gov/bill/112th-congress/senate-bill/886/text?format=txt")
scrape_page()
test
url.list <- list()
url.list <- list()
for (i in 1:100){
url.list<- c(url.list, bill_urls_fixed[i])
}
url.list <- list()
scrape_page <- function(url = bill_urls_fixed[i]){
# Print out the input name
cat(url, "\n")
# Make the input name all lowercase
url <- tolower(url)
# Downloads the web page source code
page <- getURL(url, .opts = list(ssl.verifypeer = FALSE))
# Split on newlines
page <- str_split(page,'\n')[[1]]
# Start of bill text
start <- grep("112th CONGRESS",page)
# End of bill text
end <- grep("&lt;all&gt;",page)
# Get just the text
bill_text <- page[start:end]
# Save to a named list object
bill.text.list <- c(bill.text.list, page = page, text = bill_text)
Sys.sleep(8)
# return the list
return(bill.text.list)
}
}
scrape_page <- function(url = bill_urls_fixed[i])
scrape_page <- function(url = bill_urls_fixed[i])
i<-1
scrape_page (url = bill_urls_fixed[i])
scrape_page <- function(url){
# Print out the input name
cat(url, "\n")
# Make the input name all lowercase
url <- tolower(url)
# Downloads the web page source code
page <- getURL(url, .opts = list(ssl.verifypeer = FALSE))
# Split on newlines
page <- str_split(page,'\n')[[1]]
# Start of bill text
start <- grep("112th CONGRESS",page)
# End of bill text
end <- grep("&lt;all&gt;",page)
# Get just the text
bill_text <- page[start:end]
# Save to a named list object
to_return <- list(page = page, text = bill_text)
# return the list
return(to_return)
}
i<-1
scrape_page (url = bill_urls_fixed[i])
bill.text.list <- list()
for (i in 1:3){
scrape_page (url = bill_urls_fixed[i])
}
bill.text.list <- list()
for (i in 1:3){
temp<-scrape_page (url = bill_urls_fixed[i])
bill.text.list <- c(bill.text.list, temp)
}
bill.text.list
length(bill.text.list)
save(scraped_data = bill.text.list, file = "Scraped Bill Text")
scrape_page <- function(url){
# Print out the input name
cat(url, "\n")
# Make the input name all lowercase
url <- tolower(url)
# Downloads the web page source code
page <- getURL(url, .opts = list(ssl.verifypeer = FALSE))
# Split on newlines
page <- str_split(page,'\n')[[1]]
# Start of bill text
start <- grep("112th CONGRESS",page)
# End of bill text
end <- grep("&lt;all&gt;",page)
# Get just the text
if(length(end) > 0 & length(start) > 0){
# Get just the text
print(start)
print(end)
if(!is.na(start) & !is.na(end)){
if(start < end & start > 0 & end > 0){
bill_text <- page[start:end]
}else{
bill_text <- ""
}
}else{
bill_text <- ""
}
}else{
bill_text <- ""
}
# Save to a named list object
to_return <- list(page = page, text = bill_text)
# return the list
return(to_return)
}
bill.text.list <- list()
for (i in 1:3){
temp<-scrape_page(url = bill_urls_fixed[i])
bill.text.list <- c(bill.text.list, temp)
}
bill.text.list <- vector("list",length = 100)
for (i in 1:3){
temp<-scrape_page(url = bill_urls_fixed[i])
bill.text.list[[i]] <-temp
}
bill.text.list <- vector("list",length = 100)
for (i in 1:10){
temp<-scrape_page(url = bill_urls_fixed[i])
bill.text.list[[i]] <-temp
Sys.sleep(4)
}
bill.text.list <- vector("list",length = 100)
for (i in 1:100){
temp<-scrape_page(url = bill_urls_fixed[i])
bill.text.list[[i]] <-temp
Sys.sleep(4)
}
save(scraped_data = bill.text.list, file = "Scraped Bill Text")
print(bill_urls_fixed)
string <- "inspections..#$^relocation..???!!!}{[]()"
Clean_String <- function(string){
# Lowercase
temp <- tolower(string)
# Remove everything that is not a number letter ? or !
temp <- stringr::str_replace_all(temp,"[^a-zA-Z\\s:\\?\\!]", " ")
# Shrink down to just one white space
temp <- stringr::str_replace_all(temp,"[\\s]+", " ")
# Split it
temp <- stringr::str_split(temp, " ")[[1]]
# Get rid of trailing "" if necessary
indexes <- which(temp == "")
if(length(indexes) > 0){
temp <- temp[-indexes]
}
return(temp)
}
string
Clean_String(string)
Clean_Text_Block <- function(text){
if(length(text) <= 1){
if(!is.na(text)){
temp <- stringr::str_replace_all(text,"[\\s]+", " ")
temp <- stringr::str_split(text, " ")[[1]]
}
}
Clean_Text_Block <- function(text){
if(length(text) <= 1){
if(!is.na(text)){
temp <- stringr::str_replace_all(text,"[\\s]+", " ")
temp <- stringr::str_split(text, " ")[[1]]
}
}
}
Clean_Text_Block(text)
Clean_Text_Block("hello world")
meh<-Clean_Text_Block("hello world")
Clean_Text_Block <- function(text){
if(length(text) <= 1){
if(!is.na(text)){
temp <- stringr::str_replace_all(text,"[\\s]+", " ")
temp <- stringr::str_split(text, " ")[[1]]
}
}
}
my_vector <- c(20:30)
my_vector
for(i in 1:length(my_vector)){
my_vector[i] <- sqrt(my_vector[i])
}
my_vector
textvector<-text[i]
tai<-Clean_Text_Block("my name is tai and i eat fruit every day")
tai
tai<-Clean_Text_Block("my name! is, tai and. i eat fruit? every day")
tai
test<-Clean_Text_Block(text = "hello world")
test
test<-Clean_Text_Block(text = bill.text.list)
test
for (i in 1:length(bill.text.list)) {
textvector<-bill.text.list[i]
}
textvector
textvector<- ""
textvector
class(textvector)
billdata<- data.frame c(bill.text.list)
billdata<- data.frame(c(bill.text.list))
rm(list = ls())
library(stringr)
library(scrapeR)
# Load in the bill urls -- you may need to set your working directory or alter
# the path below
load("Bill_URLs.Rdata")
# Try visiting the webiste, you will see that these URL's are from a beta
# version. The URLs will look like:
# http://beta.congress.gov/bill/112th-congress/senate-bill/886
# What we actually want is something of the form:
# https://www.congress.gov/bill/112th-congress/senate-bill/886/text?format=txt
# we will need to loop through the text and replace the beginning "http://beta."
# with "https://www." and then we will need to paste on "/text?format=txt" at
# the end of each string.
# You will want to use str_replace() and paste() functions to do this in a for()
# loop.
bill_urls_fixed <- rep("",100)
for  (i in 1:100) {
bill_urls_fixed[i]<- str_replace(Bill_URLs[i], "http://beta","https://www")
bill_urls_fixed[i]<- paste(bill_urls_fixed[i], "/text?format=txt", sep="")
}
# Once you have the right URLs, you will want to scrape the web pages. Lets
# start with a function adapted from the intermediate workshop:
scrape_page <- function(url){
# Print out the input name
cat(url, "\n")
# Make the input name all lowercase
url <- tolower(url)
# Downloads the web page source code
page <- getURL(url, .opts = list(ssl.verifypeer = FALSE))
# Split on newlines
page <- str_split(page,'\n')[[1]]
# Start of bill text
start <- grep("112th CONGRESS",page)
# End of bill text
end <- grep("&lt;all&gt;",page)
# Get just the text
if(length(end) > 0 & length(start) > 0){
# Get just the text
print(start)
print(end)
if(!is.na(start) & !is.na(end)){
if(start < end & start > 0 & end > 0){
bill_text <- page[start:end]
}else{
bill_text <- ""
}
}else{
bill_text <- ""
}
}else{
bill_text <- ""
}
# Save to a named list object
to_return <- list(page = page, text = bill_text)
# return the list
return(to_return)
}
# test it out, take a look at the
test <- scrape_page( url = "https://www.congress.gov/bill/112th-congress/senate-bill/886/text?format=txt")
# Now you will need to create a list object to store the data in, and loop over
# URLS to store the data in the list. You will probably want to save your data
# as an .Rdata object using save() at this point. One important point is that
# you NEED TO INCLUDE a Sys.sleep(5) in your scraping loop so you do not go too
# fast and overwhelm the congress.gov servers. Going too fast can land you in
# BIG legal trouble (that is called a "denial of service attack") so jsut keep
# things at a reasonable pace.
bill.text.list <- vector("list",length = 100)
for (i in 1:100){
temp<-scrape_page(url = bill_urls_fixed[i])
bill.text.list[[i]] <-temp
Sys.sleep(4)
}
rm(list = ls())
load("Scraped Bill Text")
Clean_String <- function(string){
# Lowercase
temp <- tolower(string)
# Remove everything that is not a number letter ? or !
temp <- stringr::str_replace_all(temp,"[^a-zA-Z\\s:\\?\\!]", " ")
# Shrink down to just one white space
temp <- stringr::str_replace_all(temp,"[\\s]+", " ")
# Split it
temp <- stringr::str_split(temp, " ")[[1]]
# Get rid of trailing "" if necessary
indexes <- which(temp == "")
if(length(indexes) > 0){
temp <- temp[-indexes]
}
return(temp)
}
Clean_Text_Block <- function(text){
if(length(text) <= 1){
# Check to see if there is any text at all with another conditional
if(length(text) == 0){
cat("There was no text in this bill! \n")
to_return <- list(num_tokens = 0, unique_tokens = 0, text = "")
}else{
# If there is , and only only one line of text then tokenize it
clean_text <- Clean_String(text)
num_tok <- length(clean_text)
num_uniq <- length(unique(clean_text))
to_return <- list(num_tokens = num_tok, unique_tokens = num_uniq, text = clean_text)
}
}else{
# Get rid of blank lines
indexes <- which(text == "")
if(length(indexes) > 0){
text <- text[-indexes]
}
# Loop through the lines in the text and use the append() function to
clean_text <- Clean_String(text[1])
for(i in 2:length(text)){
# add them to a vector
clean_text <- append(clean_text,Clean_String(text[i]))
}
num_tok <- length(clean_text)
num_uniq <- length(unique(clean_text))
to_return <- list(num_tokens = num_tok, unique_tokens = num_uniq, text = clean_text)
}
# Calculate the number of tokens and unique tokens and return them in a
# named list object.
return(to_return)
}
#' run on all bills
clean_bill_text <- vector(mode = "list",length = 100)
for(i in 1:100){
print(i)
clean_bill_text[[i]] <- Clean_Text_Block(bill_data[[i]]$text)
}
#' run on all bills
clean_bill_text <- vector(mode = "list",length = 100)
for(i in 1:100){
print(i)
clean_bill_text[[i]] <- Clean_Text_Block(bill.text.list[[i]]$text)
}
save(bill.text.list, file ="Scraped Bill Text.Rdata")
load ("Scraped Bill Text")
load ("Scraped Bill Text.Rdata")
Clean_String <- function(string){
# Lowercase
temp <- tolower(string)
# Remove everything that is not a number letter ? or !
temp <- stringr::str_replace_all(temp,"[^a-zA-Z\\s:\\?\\!]", " ")
# Shrink down to just one white space
temp <- stringr::str_replace_all(temp,"[\\s]+", " ")
# Split it
temp <- stringr::str_split(temp, " ")[[1]]
# Get rid of trailing "" if necessary
indexes <- which(temp == "")
if(length(indexes) > 0){
temp <- temp[-indexes]
}
return(temp)
}
#' function to clean text
Clean_Text_Block <- function(text){
if(length(text) <= 1){
# Check to see if there is any text at all with another conditional
if(length(text) == 0){
cat("There was no text in this bill! \n")
to_return <- list(num_tokens = 0, unique_tokens = 0, text = "")
}else{
# If there is , and only only one line of text then tokenize it
clean_text <- Clean_String(text)
num_tok <- length(clean_text)
num_uniq <- length(unique(clean_text))
to_return <- list(num_tokens = num_tok, unique_tokens = num_uniq, text = clean_text)
}
}else{
# Get rid of blank lines
indexes <- which(text == "")
if(length(indexes) > 0){
text <- text[-indexes]
}
# Loop through the lines in the text and use the append() function to
clean_text <- Clean_String(text[1])
for(i in 2:length(text)){
# add them to a vector
clean_text <- append(clean_text,Clean_String(text[i]))
}
num_tok <- length(clean_text)
num_uniq <- length(unique(clean_text))
to_return <- list(num_tokens = num_tok, unique_tokens = num_uniq, text = clean_text)
}
# Calculate the number of tokens and unique tokens and return them in a
# named list object.
return(to_return)
}
#' run on all bills
clean_bill_text <- vector(mode = "list",length = 100)
for(i in 1:100){
print(i)
clean_bill_text[[i]] <- Clean_Text_Block(bill.text.list[[i]]$text)
}
total_token_count <- 0
all_tokens <- NULL
for(i in 1:100){
print(i)
#keep appending the tokens to a giant vector
all_tokens <- append(all_tokens,clean_bill_text[[i]]$text)
total_token_count <- total_token_count + clean_bill_text[[i]]$num_tokens
}
unique_words <- unique(all_tokens)
total_unique_words <- length(unique_words)
cat("There were a total of",total_token_count,"tokens used in all documents and the number of unique words is:",total_unique_words," \n" )
wordcount<- function(word){
for(i in 1:100){
if grepl(word, bill.text.list[[i]]$text) {
tally<- (tally+1)
}
}
}
wordcount<- function(word){
for(i in 1:100){
if grepl(word, bill.text.list[[i]]$text) {
tally<- (tally+1)
}
}
}
wordcount<- function(word){
for(i in 1:100){
if grepl(word, bill.text.list[[i]]$text) {
tally<- (tally+1)
}
}
}
wordcount<- function(word){
tally <-0
for(i in 1:100){
if grepl(word, bill.text.list[[i]]$text) {
tally<- (tally+1)
}
}
}
wordcount<- function(word){
tally <-0
for(i in 1:100) {
if (grepl(word, bill.text.list[[i]]$text)) {
tally<- (tally+1)
}
}
}
wordcount("policy")
wordcount(word = "policy")
warnings()
wordcount<- function(word){
for(i in 1:100) {
str_count(bill.text.list[[i]]$text, word)
}
}
wordcount(word = "policy")
wordcount<- function(word){
for(i in 1:100) {
tally <- str_count(bill.text.list[[i]]$text, word)
}
return(tally)
}
wordcount(word = "policy")
fruit <- list(testing, one, two, three)
fruit <- list("testing", "one", "two", "three")
str_count(fruit, "ing")
